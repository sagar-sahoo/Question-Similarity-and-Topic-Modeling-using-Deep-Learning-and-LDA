{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\singh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from string import punctuation\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation,GlobalAveragePooling1D,Lambda,Bidirectional\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras import backend as K\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\singh\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3063: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    " = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preproprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = set(stopwords.words('english'))\n",
    "\n",
    "SPECIAL_TOKENS = {\n",
    "    'quoted': 'quoted_item',\n",
    "    'non-ascii': 'non_ascii_word',\n",
    "    'undefined': 'something'\n",
    "}\n",
    "\n",
    "def text_to_word_list(text):\n",
    "    ''' Pre process and convert texts to a list of words '''\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "    \n",
    "    def pad_str(s):\n",
    "        return ' '+s+' '\n",
    "    \n",
    "    if pd.isnull(text):\n",
    "        return ''\n",
    "\n",
    "#    stops = set(stopwords.words(\"english\"))\n",
    "    # Clean the text, with the option to stem words.\n",
    "    \n",
    "    # Empty question\n",
    "    \n",
    "    if type(text) != str or text=='':\n",
    "        return ''\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    \n",
    "    text = re.sub(r\"doesn't\", \"does not \", text)\n",
    "    text = re.sub(r\"didn't\", \"did not \", text)\n",
    "    text = re.sub(r\"wouldn't\", \"would not \", text)\n",
    "    text = re.sub(r\"isn't\", \"is not \", text)\n",
    "    \n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \"  \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    \n",
    "    text = re.sub(\"e\\.g\\.\", \" eg \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"b\\.g\\.\", \" bg \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"(\\d+)(kK)\", \" \\g<1>000 \", text)\n",
    "    text = re.sub(\"e-mail\", \" email \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"(the[\\s]+|The[\\s]+)?U\\.S\\.A\\.\", \" America \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"(the[\\s]+|The[\\s]+)?United State(s)?\", \" America \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"\\(s\\)\", \" \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"[c-fC-F]\\:\\/\", \" disk \", text)\n",
    "    \n",
    "    # remove comma between numbers, i.e. 15,000 -> 15000\n",
    "    \n",
    "    text = re.sub('(?<=[0-9])\\,(?=[0-9])', \"\", text)\n",
    "    \n",
    "    \n",
    "    text = re.sub('\\$', \" dollar \", text)\n",
    "    text = re.sub('\\%', \" percent \", text)\n",
    "    text = re.sub('\\&', \" and \", text)\n",
    "    \n",
    "    text = re.sub('[^\\x00-\\x7F]+', pad_str(SPECIAL_TOKENS['non-ascii']), text) # replace non-ascii word with special word\n",
    "    \n",
    "    # indian dollar\n",
    "    \n",
    "    text = re.sub(\"(?<=[0-9])rs \", \" rs \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\" rs(?=[0-9])\", \" rs \", text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # clean text rules get from : https://www.kaggle.com/currie32/the-importance-of-cleaning-text\n",
    "    text = re.sub(r\" (the[\\s]+|The[\\s]+)?US(A)? \", \" America \", text)\n",
    "    text = re.sub(r\" UK \", \" England \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" india \", \" India \", text)\n",
    "    text = re.sub(r\" switzerland \", \" Switzerland \", text)\n",
    "    text = re.sub(r\" china \", \" China \", text)\n",
    "    text = re.sub(r\" chinese \", \" Chinese \", text) \n",
    "    text = re.sub(r\" imrovement \", \" improvement \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" intially \", \" initially \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" quora \", \" Quora \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" dms \", \" direct messages \", text, flags=re.IGNORECASE)  \n",
    "    text = re.sub(r\" demonitization \", \" demonetization \", text, flags=re.IGNORECASE) \n",
    "    text = re.sub(r\" actived \", \" active \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" kms \", \" kilometers \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" cs \", \" computer science \", text, flags=re.IGNORECASE) \n",
    "    text = re.sub(r\" upvote\", \" up vote\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" iPhone \", \" phone \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" \\0rs \", \" rs \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" calender \", \" calendar \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" ios \", \" operating system \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" gps \", \" GPS \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" gst \", \" GST \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" programing \", \" programming \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" bestfriend \", \" best friend \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" dna \", \" DNA \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" III \", \" 3 \", text)\n",
    "    text = re.sub(r\" banglore \", \" Banglore \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" J K \", \" JK \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" J\\.K\\. \", \" JK \", text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # replace the float numbers with a random number, it will be parsed as number afterward, and also been replaced with word \"number\"\n",
    "    \n",
    "    text = re.sub('[0-9]+\\.[0-9]+', \" 87 \", text)\n",
    "    \n",
    "    \n",
    "\n",
    "    text = text.split()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tokenization.pkl\", 'rb') as file:\n",
    "    tokenizer = pickle.load(file)\n",
    "    \n",
    "with open(\"embedding_matrix.pkl\", 'rb') as file:\n",
    "    embedding_matrix = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_processing(data):\n",
    "    q1 = data[1]\n",
    "    q2 = data[2]\n",
    "    \n",
    "    q1_transf = text_to_word_list(q1)\n",
    "    q2_transf = text_to_word_list(q2)\n",
    "    \n",
    "    q1_token = tokenizer.texts_to_sequences(q1_transf)\n",
    "    q2_token = tokenizer.texts_to_sequences(q2_transf)\n",
    "    \n",
    "    return pad_sequences([[j[0] for j in q1_token if len(j) != 0], [i[0] for i in q2_token if len(i) != 0]], maxlen=25) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining embedding layer\n",
    "nb_words = embedding_matrix.shape[0]                 \n",
    "\n",
    "max_sentence_len = 25\n",
    "\n",
    "embedding_layer = Embedding(nb_words,300,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_sentence_len,\n",
    "                            trainable=False)\n",
    "#dont train this layer!\n",
    "\n",
    "\n",
    "#defining function to find similarity through vector distance \n",
    "def vec_distance(vects):\n",
    "    x, y = vects\n",
    "    return K.sum(K.square(x - y), axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "def vec_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\singh\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n"
     ]
    }
   ],
   "source": [
    "lstm_layer =LSTM(128)\n",
    "\n",
    "sequence_1_input = Input(shape=(max_sentence_len,), dtype='int32')\n",
    "embedded_sequences_1 = embedding_layer(sequence_1_input)\n",
    "x1 = lstm_layer(embedded_sequences_1)\n",
    "\n",
    "sequence_2_input = Input(shape=(max_sentence_len,), dtype='int32')\n",
    "embedded_sequences_2 = embedding_layer(sequence_2_input)\n",
    "y1 = lstm_layer(embedded_sequences_2)\n",
    "\n",
    "distance=Lambda(vec_distance, output_shape=vec_output_shape)([x1, y1])\n",
    "\n",
    "dense1 = Dense(16, activation='sigmoid')(distance)\n",
    "dense1 = Dropout(0.3)(dense1)\n",
    "\n",
    "bn2 = BatchNormalization()(dense1)\n",
    "\n",
    "prediction=Dense(1, activation='sigmoid')(bn2)\n",
    "\n",
    "model = Model(input=[sequence_1_input, sequence_2_input], output=prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('trained_model_ml_similarity.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_semantic(i):\n",
    "    data = test_df.iloc[i]\n",
    "    data_new = text_processing(data)\n",
    "    q1 = data_new[0].reshape(-1,25)\n",
    "    q2 = data_new[1].reshape(-1,25)\n",
    "    \n",
    "    print('\\nQuestion 1 -', data[1])\n",
    "    \n",
    "    print('\\nQuestion 2 -', data[2])\n",
    "    \n",
    "    print(\"\\nPrediction from the model \", 1*(model.predict([q1,q2])[0][0] >= 0.5))\n",
    "    return None "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"corpus.pkl\", 'rb') as file:\n",
    "    corpus = pickle.load(file)\n",
    "    \n",
    "with open(\"lda_model.pkl\", 'rb') as file:\n",
    "    lda_model = pickle.load(file)\n",
    "    \n",
    "with open(\"dfques.pkl\", 'rb') as file:\n",
    "    dfques = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topics(i):\n",
    "    index = i*2 + random.randint(1, 2)\n",
    "    question = dfques[index]\n",
    "    print(\"\\nQuestion -\", question)\n",
    "    \n",
    "    a = []\n",
    "    b = []\n",
    "    for i, j in lda_model[corpus[0]]:\n",
    "        a.append(i)\n",
    "        b.append(j)\n",
    "    \n",
    "    print(\"\\nTopic distribution is -\")\n",
    "    print(pd.Series(b, index = ['Topic_1', 'Topic_2', 'Topic_3', 'Topic_4', 'Topic_5', 'Topic_6', 'Topic_7', 'Topic_8', 'Topic_9', 'Topic_10']))\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_id</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>How does the Surface Pro himself 4 compare wit...</td>\n",
       "      <td>Why did Microsoft choose core m3 and not core ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Should I have a hair transplant at age 24? How...</td>\n",
       "      <td>How much cost does hair transplant require?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>What but is the best way to send money from Ch...</td>\n",
       "      <td>What you send money to China?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Which food not emulsifiers?</td>\n",
       "      <td>What foods fibre?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>How \"aberystwyth\" start reading?</td>\n",
       "      <td>How their can I start reading?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  test_id                                          question1  \\\n",
       "0       0  How does the Surface Pro himself 4 compare wit...   \n",
       "1       1  Should I have a hair transplant at age 24? How...   \n",
       "2       2  What but is the best way to send money from Ch...   \n",
       "3       3                        Which food not emulsifiers?   \n",
       "4       4                   How \"aberystwyth\" start reading?   \n",
       "\n",
       "                                           question2  \n",
       "0  Why did Microsoft choose core m3 and not core ...  \n",
       "1        How much cost does hair transplant require?  \n",
       "2                      What you send money to China?  \n",
       "3                                  What foods fibre?  \n",
       "4                     How their can I start reading?  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Similarity Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter question number (between 0 and 2345796) - 56\n",
      "\n",
      "Question 1 - Which is a better watch brand Chopard vs Cartier?\n",
      "\n",
      "Question 2 - Which watch brand has the most prestige?\n",
      "\n",
      "Prediction from the model  0\n"
     ]
    }
   ],
   "source": [
    "i = int(input(\"Please enter question number (between 0 and 2345796) - \"))\n",
    "\n",
    "prediction_semantic(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter question number - 67\n",
      "\n",
      "Question - What is your review of Performance Testing?\n",
      "\n",
      "Topic distribution is -\n",
      "Topic_1     0.362430\n",
      "Topic_2     0.033344\n",
      "Topic_3     0.033339\n",
      "Topic_4     0.370840\n",
      "Topic_5     0.033344\n",
      "Topic_6     0.033339\n",
      "Topic_7     0.033339\n",
      "Topic_8     0.033343\n",
      "Topic_9     0.033340\n",
      "Topic_10    0.033342\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "i = int(input(\"Please enter question number - \"))\n",
    "\n",
    "topics(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.015*\"instagram\" + 0.014*\"bank\" + 0.014*\"food\" + 0.012*\"process\" + 0.010*\"considered\" + 0.009*\"air\" + 0.009*\"seen\" + 0.009*\"traffic\" + 0.008*\"battle\" + 0.008*\"videos\"'),\n",
       " (1,\n",
       "  '0.024*\"facebook\" + 0.013*\"travel\" + 0.012*\"rid\" + 0.011*\"whatsapp\" + 0.010*\"school\" + 0.010*\"email\" + 0.010*\"body\" + 0.009*\"code\" + 0.009*\"fat\" + 0.009*\"delete\"'),\n",
       " (2,\n",
       "  '0.014*\"student\" + 0.012*\"safe\" + 0.011*\"university\" + 0.010*\"tech\" + 0.010*\"engineer\" + 0.009*\"students\" + 0.009*\"mechanical\" + 0.009*\"police\" + 0.009*\"hotel\" + 0.008*\"major\"'),\n",
       " (3,\n",
       "  '0.025*\"sex\" + 0.019*\"learning\" + 0.013*\"interview\" + 0.012*\"chinese\" + 0.011*\"video\" + 0.011*\"culture\" + 0.010*\"answers\" + 0.010*\"stop\" + 0.010*\"companies\" + 0.009*\"design\"'),\n",
       " (4,\n",
       "  '0.027*\"ways\" + 0.018*\"system\" + 0.017*\"top\" + 0.016*\"earn\" + 0.015*\"mobile\" + 0.011*\"java\" + 0.011*\"interesting\" + 0.009*\"service\" + 0.009*\"type\" + 0.009*\"parents\"'),\n",
       " (5,\n",
       "  '0.019*\"iphone\" + 0.014*\"2017\" + 0.013*\"place\" + 0.011*\"word\" + 0.011*\"class\" + 0.011*\"science\" + 0.009*\"gmail\" + 0.009*\"bangalore\" + 0.008*\"sentence\" + 0.008*\"death\"'),\n",
       " (6,\n",
       "  '0.033*\"english\" + 0.019*\"movies\" + 0.015*\"math\" + 0.013*\"meaning\" + 0.013*\"come\" + 0.013*\"important\" + 0.012*\"watch\" + 0.012*\"average\" + 0.010*\"skills\" + 0.010*\"god\"'),\n",
       " (7,\n",
       "  '0.015*\"country\" + 0.015*\"energy\" + 0.012*\"favorite\" + 0.011*\"hair\" + 0.009*\"today\" + 0.009*\"deal\" + 0.008*\"california\" + 0.008*\"dark\" + 0.008*\"sleep\" + 0.007*\"created\"'),\n",
       " (8,\n",
       "  '0.029*\"500\" + 0.029*\"notes\" + 0.022*\"weight\" + 0.017*\"war\" + 0.016*\"men\" + 0.012*\"rupee\" + 0.011*\"modi\" + 0.011*\"age\" + 0.008*\"boyfriend\" + 0.008*\"center\"'),\n",
       " (9,\n",
       "  '0.038*\"trump\" + 0.024*\"donald\" + 0.016*\"clinton\" + 0.015*\"hillary\" + 0.015*\"win\" + 0.013*\"social\" + 0.011*\"car\" + 0.009*\"election\" + 0.009*\"exist\" + 0.009*\"mind\"')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.print_topics(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
